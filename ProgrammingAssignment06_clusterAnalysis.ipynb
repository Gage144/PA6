{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "084a5836-46c8-47ac-a1e0-e8348a525150",
      "cell_type": "markdown",
      "source": "# ProgrammingAssignment06_clusterAnalysis ",
      "metadata": {}
    },
    {
      "id": "c7bf8149-9aa3-4627-8f2c-49e531f9c5e9",
      "cell_type": "markdown",
      "source": "## 1. k-means using scikit-learn\nThe healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply k-means clustering to the cities' number of hours of sunshine and happiness levels.\n\n- Import the needed packages for clustering.\n- Initialize and fit a k-means clustering model using sklearn's Kmeans() function.\n- Use the user-defined number of clusters, init='random', n_init=10, random_state=123, and algorithm='elkan'.\n- Find the cluster centroids and inertia.\n\nEx: If the input is: 4\n\nthe output should be:\n\n- Centroids: [[ 0.8294  0.2562]\n [ 1.3106 -1.887 ]\n [-0.9471  0.8281]\n [-0.6372 -0.7943]]\n- Inertia: 16.4991",
      "metadata": {}
    },
    {
      "id": "ad54189b-81b1-4a58-9dd8-42ca5ba05310",
      "cell_type": "code",
      "source": "# Import needed packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\nhealthy = pd.read_csv('healthy_lifestyle.csv')\n\n# Set the number of clusters directly (replace the input() function)\nnumber = 5  # Change this value as needed\n\n# Define input features\nX = healthy[['sunshine_hours', 'happiness_levels']]\n\n# Use StandardScaler() to standardize input features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\nX = X.dropna()\n\n# Initialize a k-means clustering algorithm with the specified parameters\nkmeans = KMeans(n_clusters=number, init='random', n_init=10, random_state=123, algorithm='elkan')\n\n# Fit the algorithm to the input features\nkmeans.fit(X)\n\n# Find and print the cluster centroids\ncentroid = kmeans.cluster_centers_\nprint(\"Centroids:\", np.round(centroid, 4))\n\n# Find and print the cluster inertia\ninertia = kmeans.inertia_\nprint(\"Inertia:\", np.round(inertia, 4))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Centroids: [[-0.6372 -0.7943]\n [-0.9471  0.8281]\n [ 1.3106 -1.887 ]\n [ 1.7175  0.5766]\n [ 0.6073  0.176 ]]\nInertia: 13.1557\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "id": "18700cd6-8069-458a-8822-61f755fd893e",
      "cell_type": "markdown",
      "source": "## 2. Hierarchical clustering using scikit-learn\nThe healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply agglomerative clustering to the cities' number of hours of sunshine and happiness levels using both sklearn and SciPy.\n\n- Import the needed packages for agglomerative clustering from sklearn and SciPy.\n- Initialize and fit an agglomerative clustering model using sklearn's AgglomerativeClustering() function. Use the user-defined number of clusters and ward linkage.\n- Add cluster labels to the input feature dataframe.\n- Calculate the distances between all instances using SciPy's pdist() function.\n- Convert the distance matrix to a square matrix using SciPy's squareform() function.\n- Define a clustering model with ward linkage using SciPy's linkage() function.\n\nEx: If the input is: 4\n\nthe output should be:\n|       | sunshine_hours | happiness_levels | labels |\n|-------|----------------|------------------|--------|\n| 0     | -0.691660      | 1.025642         | 3      |\n| 1     | 0.695725       | 0.801124         | 0      |\n| 2     | -0.645295      | 0.872562         | 3      |\n| 3     | -0.757641      | 0.933794         | 3      |\n| 4     | -1.098246      | 1.229750         | 3      |\n\n\nFirst five rows of the linkage matrix from SciPy:\n    \n - [[39. 40.  0.  2.]\n [28. 43.  0.  3.]\n [ 7. 18.  0.  2.]\n [ 8. 42.  0.  2.]\n [ 0.  3.  0.  2.]]",
      "metadata": {}
    },
    {
      "id": "f58d1471-d33e-43b5-839c-ba86c9509ebc",
      "cell_type": "code",
      "source": "# Import needed packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage\n\n# Silence warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the dataset\nhealthy = pd.read_csv('healthy_lifestyle.csv')\n\n# Set the number of clusters directly (replace the input() function)\nnumber = 3  # Change this value as needed\n\n# Define input features\nX = healthy[['sunshine_hours', 'happiness_levels']]\n\n# Use StandardScaler() to standardize input features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=['sunshine_hours', 'happiness_levels'])\nX_scaled = X_scaled.dropna()\n\n# Initialize and fit an agglomerative clustering model using ward linkage in scikit-learn, with the specified number of clusters\nagglo_model = AgglomerativeClustering(n_clusters=number, linkage='ward')\nlabels = agglo_model.fit_predict(X_scaled)\n\n# Add cluster labels to input feature dataframe\nX_scaled['labels'] = labels\nprint(\"Clustered DataFrame:\\n\", X_scaled.head())\n\n# Perform agglomerative clustering using SciPy\n\n# Calculate the distances between all instances\ndistances = pdist(X_scaled[['sunshine_hours', 'happiness_levels']])\n\n# Convert the distance matrix to a square matrix\ndistance_matrix = squareform(distances)\n\n# Define a clustering model with ward linkage\nclustersHealthyScipy = linkage(distances, method='ward')\n\nprint('First five rows of the linkage matrix from SciPy:\\n', np.round(clustersHealthyScipy[:5, :], 0))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Clustered DataFrame:\n    sunshine_hours  happiness_levels  labels\n0       -0.691660          1.025642       0\n1        0.695725          0.801124       1\n2       -0.645295          0.872562       0\n3       -0.757641          0.933794       0\n4       -1.098246          1.229750       0\nFirst five rows of the linkage matrix from SciPy:\n [[39. 40.  0.  2.]\n [28. 43.  0.  3.]\n [ 7. 18.  0.  2.]\n [ 8. 42.  0.  2.]\n [36. 45.  0.  3.]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 17
    },
    {
      "id": "78db51dd-7ad3-4f58-b0ff-ae71a6c306e8",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d92b04d3-05bd-470c-a199-327a6b55679c",
      "cell_type": "markdown",
      "source": "## 3. DBSCAN using scikit-learn\n- Increase the **number of points sampled to 500**.\n- Apply the DBSCAN model with **epsilon=1** and **min_samples=8** to identify the number of core-points and outliers (or noise). \n- EX: if the epsilon=1 and min_samples = 10 and number of points sampled to 100.\n  - the number of core-points = 85\n  - the number of outliers    = 11",
      "metadata": {}
    },
    {
      "id": "fdeae7b7-d4e7-483a-81bc-1d4bd4d6716c",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\n# Load the dataset and take a random sample of 500 instances\ndata = pd.read_csv('customer_personality.csv').sample(500, random_state=123)\n\n# Use StandardScaler() to standardize input features\nX = data[['Fruits', 'Meats']]\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['Fruits', 'Meats'])\n\n# Apply DBSCAN with epsilon=1 and min_samples=8\ndbscan = DBSCAN(eps=1, min_samples=8)\ndbscan = dbscan.fit(X)\n\n# Identify core points and outliers\nlabels = dbscan.labels_\ncore_points = np.sum(labels != -1)  # All points that are not labeled as -1 are core points or cluster members\noutliers = np.sum(labels == -1)  # All points labeled as -1 are outliers (noise)\n\nprint(\"Number of core points:\", core_points)\nprint(\"Number of outliers:\", outliers)\n\n# Add the cluster labels to the dataset as strings\ndata['clusters'] = labels.astype(str)\n\n# Sort by cluster label (for plotting purposes)\ndata.sort_values(by='clusters', inplace=True)\n\n# Plot clusters on the original data\nplt.figure(figsize=(10, 6))\np = sns.scatterplot(data=data, x='Fruits', y='Meats', hue='clusters', style='clusters')\np.set_xlabel('Fruits', fontsize=16)\np.set_ylabel('Meats', fontsize=16)\np.legend(title='DBSCAN')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'seaborn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "3ea545b6-c934-42ca-9f5a-be7bf69d22d9",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}